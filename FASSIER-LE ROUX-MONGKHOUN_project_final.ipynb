{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Sport Image Classification**","metadata":{}},{"cell_type":"markdown","source":"### Team Members :\n- FASSIER Thimoth√©\n- LE ROUX Thomas\n- MONGKHOUN Vincent\n\nIn this project, we are going to build a sport-image classifier. The model will take as an input an image and will return as an output the sport that is represented in it.","metadata":{}},{"cell_type":"markdown","source":"During this project, we have used some fonctions that were introduced during lecture labs about CNN and Computer Vision of this course.","metadata":{}},{"cell_type":"markdown","source":"This notebook of the project is divided into 4 parts :\n- **Data Exploration and Data Pre-processing**\n- **Transfer Learning with CNN features extraction**\n- **Transfer Learning with Fine Tuning**","metadata":{}},{"cell_type":"markdown","source":"# Importation of packages","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\nimport keras\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport random\nimport torch.nn.functional as F\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport cv2\n\nfrom IPython.display import Image\nimport matplotlib.cm as cm\n\nimport os\nimport shutil","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-27T14:15:02.628097Z","iopub.execute_input":"2022-03-27T14:15:02.628519Z","iopub.status.idle":"2022-03-27T14:15:10.200867Z","shell.execute_reply.started":"2022-03-27T14:15:02.628408Z","shell.execute_reply":"2022-03-27T14:15:10.200155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading the dataset","metadata":{}},{"cell_type":"markdown","source":"If the dataset is not already dowloaded, follow the following steps : \n- Download the datset with the following link : https://www.kaggle.com/datasets/rishikeshkonapure/sports-image-dataset/download\n- Unzip and place the dataset in the same folder as this notebook","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration and Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"## Test-data creation","metadata":{}},{"cell_type":"markdown","source":"The original dataset doesn't provide a folder where test data are located. Consequently, before beginning the data exploration and pre-precossing, we need to allocate some images as test data for the testing phase of our trained model","metadata":{}},{"cell_type":"code","source":"environment = 'kaggle'\n\nif environment == 'kaggle' : \n    DATASET_DIR = '../input/sports-image-dataset/sports-image-dataset'\nelif environment == 'colab':\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATASET_DIR = '/content/drive/My Drive/Applied DL Project/sports-image-dataset/'\nelse :\n    DATASET_DIR = './sports-image-dataset'\n\nDATA_DIR = os.path.join(DATASET_DIR, 'data')\nsports = os.listdir(DATA_DIR)\n\nprint('The sports that our model will classify are the following :')\nprint(sports)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:10.202719Z","iopub.execute_input":"2022-03-27T14:15:10.20298Z","iopub.status.idle":"2022-03-27T14:15:10.217466Z","shell.execute_reply.started":"2022-03-27T14:15:10.202946Z","shell.execute_reply":"2022-03-27T14:15:10.216781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let create a test data subfolder to store the data that will be used for the testing phase. We randomly select the data from each classes ","metadata":{}},{"cell_type":"code","source":"# Creation of the test subfolder and the dataset\n\nn_validation = 100\n\ntest_folder = os.path.join(DATASET_DIR, 'test_data')\nif not os.path.exists(test_folder):\n    os.mkdir(test_folder)\n    for class_name in sports:\n        train_subfolder = os.path.join(DATA_DIR, class_name)\n        test_subfolder = os.path.join(test_folder, class_name)\n        print(\"Populating %s...\" % test_subfolder)\n        os.mkdir(test_subfolder)\n        images_filenames = sorted(os.listdir(train_subfolder))\n        for image_filename in images_filenames[-n_validation:]:\n            shutil.move(os.path.join(train_subfolder, image_filename),\n                        test_subfolder)\n        print(\"Moved %d images\" % len(os.listdir(test_subfolder)))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:10.21911Z","iopub.execute_input":"2022-03-27T14:15:10.219607Z","iopub.status.idle":"2022-03-27T14:15:10.227637Z","shell.execute_reply.started":"2022-03-27T14:15:10.219559Z","shell.execute_reply":"2022-03-27T14:15:10.226952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creation of the tensorflow-datasets","metadata":{}},{"cell_type":"markdown","source":"As we have a set of images from our dataset that are filed into class-specific folders, we will use the method <code>tf.keras.utils.image_dataset_from_directory</code> to automatically process the data directly from the train subfolder and also to generate similar labeled dataset objects. \n\nWe divide the data into 80% for the train dataset and 20% for the validation dataset.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nimg_height = 224\nimg_width = 224","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:10.229882Z","iopub.execute_input":"2022-03-27T14:15:10.230377Z","iopub.status.idle":"2022-03-27T14:15:10.235464Z","shell.execute_reply.started":"2022-03-27T14:15:10.230343Z","shell.execute_reply":"2022-03-27T14:15:10.234783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.keras.utils.image_dataset_from_directory(\n  DATA_DIR,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n  color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:10.236881Z","iopub.execute_input":"2022-03-27T14:15:10.237247Z","iopub.status.idle":"2022-03-27T14:15:17.130454Z","shell.execute_reply.started":"2022-03-27T14:15:10.237208Z","shell.execute_reply":"2022-03-27T14:15:17.129705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = tf.keras.utils.image_dataset_from_directory(\n  DATA_DIR,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n  color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:17.131838Z","iopub.execute_input":"2022-03-27T14:15:17.132299Z","iopub.status.idle":"2022-03-27T14:15:17.803222Z","shell.execute_reply.started":"2022-03-27T14:15:17.132261Z","shell.execute_reply":"2022-03-27T14:15:17.802521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can check if everything is good by looking at the shape of one batch","metadata":{}},{"cell_type":"code","source":"# Check how the images are arranged within a batch \nfor image_batch, labels_batch in train_dataset:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:17.804499Z","iopub.execute_input":"2022-03-27T14:15:17.805281Z","iopub.status.idle":"2022-03-27T14:15:19.062658Z","shell.execute_reply.started":"2022-03-27T14:15:17.805239Z","shell.execute_reply":"2022-03-27T14:15:19.061822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Everything seems to be good as a batch of size 32 and the images are correctly converted into (img_height, img_width, nb_channels) format.","metadata":{}},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"class_names = train_dataset.class_names\nnb_sports = len(class_names)\nprint('We will classify the folliwing sports : ' + str(class_names))\nprint('There is a total of ' + str(nb_sports) + ' sports')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:19.064044Z","iopub.execute_input":"2022-03-27T14:15:19.064325Z","iopub.status.idle":"2022-03-27T14:15:19.069985Z","shell.execute_reply.started":"2022-03-27T14:15:19.064288Z","shell.execute_reply":"2022-03-27T14:15:19.069193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to quickly switch between the name of the sport and the corresponding label, we define the following dictionay :","metadata":{}},{"cell_type":"code","source":"# Create a quick path between label and sport\nINDEX_TO_CLASS = {k: v for k, v in enumerate(class_names)}\nINDEX_TO_CLASS","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:19.071369Z","iopub.execute_input":"2022-03-27T14:15:19.071755Z","iopub.status.idle":"2022-03-27T14:15:19.084258Z","shell.execute_reply.started":"2022-03-27T14:15:19.071718Z","shell.execute_reply":"2022-03-27T14:15:19.083495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now plot one image of each class to see what it's look like","metadata":{}},{"cell_type":"code","source":"# Plot one image per class\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n    for i in range(22):\n        ax = plt.subplot(5, 5, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:19.087662Z","iopub.execute_input":"2022-03-27T14:15:19.088193Z","iopub.status.idle":"2022-03-27T14:15:20.650727Z","shell.execute_reply.started":"2022-03-27T14:15:19.088155Z","shell.execute_reply":"2022-03-27T14:15:20.650101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now see what the images from the same class looks like. We first define a function that plot the different images from the same class","metadata":{}},{"cell_type":"code","source":"def plot_classes(random_sport):\n    '''Plot images from the same class'''\n\n    n_rows = 3\n    n_cols = 4\n    sport_folder = os.path.join(DATA_DIR, random_sport)\n    list_img = os.listdir(sport_folder)\n    \n    for row in range(n_rows):\n        for col in range(n_cols):\n            index = n_cols * row + col\n            plt.subplot(n_rows, n_cols, index+1)\n\n            # Pick a random images to plot\n            for i in range(8):\n                x = random.randint(0, len(list_img) - 2)\n                image_ = plt.imread(os.path.join(sport_folder, list_img[x]))\n                # Display the image\n                plt.imshow(image_, cmap='binary', interpolation='nearest')\n                plt.title(random_sport)\n                plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:20.651615Z","iopub.execute_input":"2022-03-27T14:15:20.651854Z","iopub.status.idle":"2022-03-27T14:15:20.662166Z","shell.execute_reply.started":"2022-03-27T14:15:20.65182Z","shell.execute_reply":"2022-03-27T14:15:20.661048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_sport = INDEX_TO_CLASS[random.randint(0, nb_sports)]\nprint(f'The selected sport to display various images is {random_sport.upper()}')\n\nplot_classes(random_sport)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:20.663685Z","iopub.execute_input":"2022-03-27T14:15:20.663964Z","iopub.status.idle":"2022-03-27T14:15:22.595874Z","shell.execute_reply.started":"2022-03-27T14:15:20.663933Z","shell.execute_reply":"2022-03-27T14:15:22.595166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_sport = INDEX_TO_CLASS[random.randint(0, nb_sports)]\nprint(f'The selected sport to display various images is {random_sport.upper()}')\n\nplot_classes(random_sport)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:22.597312Z","iopub.execute_input":"2022-03-27T14:15:22.597566Z","iopub.status.idle":"2022-03-27T14:15:24.660624Z","shell.execute_reply.started":"2022-03-27T14:15:22.59753Z","shell.execute_reply":"2022-03-27T14:15:24.660013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_sport = INDEX_TO_CLASS[random.randint(0, nb_sports)]\nprint(f'The selected sport to display various images is {random_sport.upper()}')\n\nplot_classes(random_sport)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:24.66167Z","iopub.execute_input":"2022-03-27T14:15:24.662026Z","iopub.status.idle":"2022-03-27T14:15:26.683657Z","shell.execute_reply.started":"2022-03-27T14:15:24.661988Z","shell.execute_reply":"2022-03-27T14:15:26.682977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remark that within the same class, images are quiet different in term of orientation, color, zoom range or even light levels. There are even images of the equipment only, such as for hockey, basketball, badminton or football. The model should be agnostic or robust to these variations.  The idea of using data augmention becomes pertinent, and it's all the more the case as the train dataset is quite small for this type of case (Image Classification).\n\n\n**Remark :** the images are not in the same size here but there will have the same during the training (cf Creation of Tensorflow Dataset section)","metadata":{}},{"cell_type":"markdown","source":"Let's check the number of images per each classes","metadata":{}},{"cell_type":"code","source":"effectifs = []\nfor sport in class_names:\n    effectif = os.listdir(os.path.join(DATA_DIR, sport))\n    effectifs.append(len(effectif))\n\nplt.figure(figsize=(25, 10))\nsns.barplot(x=class_names, y=effectifs, palette='viridis')\nplt.title('Images per Sports',)\nplt.ylabel('Number of images')\nplt.xlabel('Sports Name')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:26.684921Z","iopub.execute_input":"2022-03-27T14:15:26.685661Z","iopub.status.idle":"2022-03-27T14:15:27.098057Z","shell.execute_reply.started":"2022-03-27T14:15:26.685621Z","shell.execute_reply":"2022-03-27T14:15:27.09739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 2 sports that are the most represented in the dataset are firstly Badminton and secondly Football. On the contrary, the ones that are the less represented are Basketball, Kabaddi and Chess. ","metadata":{}},{"cell_type":"markdown","source":"## Data-Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We first configure the performance of the dataset.\n\nThe <code>Dataset.cache</code> keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training our model.\n\nThe <code>Dataset.prefetch</code> overlaps data preprocessing and model execution while training.","metadata":{}},{"cell_type":"code","source":"train_dataset = train_dataset.cache().prefetch(buffer_size=10)\nval_dataset = val_dataset.cache().prefetch(buffer_size=10)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:27.099171Z","iopub.execute_input":"2022-03-27T14:15:27.099975Z","iopub.status.idle":"2022-03-27T14:15:27.108119Z","shell.execute_reply.started":"2022-03-27T14:15:27.099936Z","shell.execute_reply":"2022-03-27T14:15:27.107296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can noww configure the different data augmentation process that we will use during the training. Regarding the different variation of images within the same class, we have decided to do : \n- Horizontal and Vertical <code>RandomFlip</code>\n- <code>RandomRotation</code> ","metadata":{}},{"cell_type":"code","source":"# Create the random data augmentation \ndata_augmentation = tf.keras.Sequential(\n    [tf.keras.layers.RandomFlip(\"horizontal\"),\n     tf.keras.layers.RandomRotation(0.1),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:27.110782Z","iopub.execute_input":"2022-03-27T14:15:27.111681Z","iopub.status.idle":"2022-03-27T14:15:27.142243Z","shell.execute_reply.started":"2022-03-27T14:15:27.111639Z","shell.execute_reply":"2022-03-27T14:15:27.141531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can test if everithing works well for one image","metadata":{}},{"cell_type":"code","source":"for images, labels in train_dataset.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = images[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(\n            tf.expand_dims(first_image, 0), training=True)\n        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n        plt.title(INDEX_TO_CLASS[int(labels[0])])\n        plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:27.143552Z","iopub.execute_input":"2022-03-27T14:15:27.143901Z","iopub.status.idle":"2022-03-27T14:15:28.482495Z","shell.execute_reply.started":"2022-03-27T14:15:27.143859Z","shell.execute_reply":"2022-03-27T14:15:28.481187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning with CNN Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"In this section, we will implement a Transfer Learning model. We will use a pretrained CNN in order to process feature extraction. The aim is to, from a previous network, extract meaningful features from new samples. We will finally add a new model on top of this pretrained model so that we can repurpose the feature maps learned previously for the dataset.","metadata":{}},{"cell_type":"markdown","source":"## The model","metadata":{}},{"cell_type":"markdown","source":"The CNN that we are using is the <code>ResNet50</code> model, which is a quite good model that present a good balance between performance and time of computation\n\nFor feature extraction, we specify <code>the include_top=False</code> argument, so that when we load the network, it doesn't include the classification layers at the top. ","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.resnet50.ResNet50(\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n    input_shape=(224, 224, 3),\n    include_top=False,\n)  ","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:28.483911Z","iopub.execute_input":"2022-03-27T14:15:28.484343Z","iopub.status.idle":"2022-03-27T14:15:30.51671Z","shell.execute_reply.started":"2022-03-27T14:15:28.484308Z","shell.execute_reply":"2022-03-27T14:15:30.515985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to prevent the weights in a given layer of the <code>based_model</code> from being updated during training, so we freeze it : ","metadata":{}},{"cell_type":"code","source":"# Freeze the base_model\nbase_model.trainable = False\n\n# Let's see at the base model architecture\nbase_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:30.518049Z","iopub.execute_input":"2022-03-27T14:15:30.518324Z","iopub.status.idle":"2022-03-27T14:15:30.601811Z","shell.execute_reply.started":"2022-03-27T14:15:30.518288Z","shell.execute_reply":"2022-03-27T14:15:30.601175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we have freezed the original top layer of the <code>ResNet50</code>, we can add our own classifier on the top of the model so it will be ready to be train with our own data.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import preprocess_input\n\n# Create new model on top\ninputs = tf.keras.Input(shape=(224, 224, 3))\n\n# Data augmentation\nx = data_augmentation(inputs)\n\n# Pre-processing the data in order to be fed to the model\nx = preprocess_input(x)\n\nx = base_model(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\n# Regularization\nx = keras.layers.Dropout(0.2)(x)\n\noutputs = keras.layers.Dense(22, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs, outputs)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:30.603088Z","iopub.execute_input":"2022-03-27T14:15:30.603349Z","iopub.status.idle":"2022-03-27T14:15:31.136218Z","shell.execute_reply.started":"2022-03-27T14:15:30.603315Z","shell.execute_reply":"2022-03-27T14:15:31.135455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now proceed to the training. We have decided to choose the following parameters : \n- <code>SGD</code> optimizer with default value of learning rate\n- <code>SparseCategoricalCrossentropy</code> loss as we didn't onehot encoded our labels and that we 22 labels and not 2 (otherwise it we be <code>CategoricalCrossentropy</code>)\n- <code>SparseCategoricalAccuracy</code> metric ","metadata":{}},{"cell_type":"code","source":"# Definition of the optimization parameters and of the metric\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.SGD(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:31.137312Z","iopub.execute_input":"2022-03-27T14:15:31.138012Z","iopub.status.idle":"2022-03-27T14:15:31.156155Z","shell.execute_reply.started":"2022-03-27T14:15:31.137971Z","shell.execute_reply":"2022-03-27T14:15:31.155426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\n\ninitial_epochs = 20\nhistory_tl = model.fit(train_dataset,\n                       epochs=initial_epochs,\n                       validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:15:31.157414Z","iopub.execute_input":"2022-03-27T14:15:31.157678Z","iopub.status.idle":"2022-03-27T14:26:14.658099Z","shell.execute_reply.started":"2022-03-27T14:15:31.157645Z","shell.execute_reply":"2022-03-27T14:26:14.657399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Perfomance of the model","metadata":{}},{"cell_type":"markdown","source":"### Loss and Accuracy","metadata":{}},{"cell_type":"markdown","source":"We can now look at learning curves of the training and validation accuracy/loss","metadata":{}},{"cell_type":"code","source":"acc = history_tl.history['sparse_categorical_accuracy']\nval_acc = history_tl.history['val_sparse_categorical_accuracy']\n\nloss = history_tl.history['loss']\nval_loss = history_tl.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:26:14.659461Z","iopub.execute_input":"2022-03-27T14:26:14.659707Z","iopub.status.idle":"2022-03-27T14:26:14.979641Z","shell.execute_reply.started":"2022-03-27T14:26:14.659671Z","shell.execute_reply":"2022-03-27T14:26:14.978966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction of new samples and analysis","metadata":{}},{"cell_type":"code","source":"TEST_DATA = os.path.join(DATASET_DIR, 'test_data')\n\ntest_dataset = tf.keras.utils.image_dataset_from_directory(\n    TEST_DATA,\n    image_size=(img_height, img_width),\n    color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:26:14.980929Z","iopub.execute_input":"2022-03-27T14:26:14.981197Z","iopub.status.idle":"2022-03-27T14:26:15.422739Z","shell.execute_reply.started":"2022-03-27T14:26:14.981163Z","shell.execute_reply":"2022-03-27T14:26:15.421999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntrue_labels = []\nimgs = []\n\nfor X, y in test_dataset :\n    image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n    pred = model.predict_on_batch(image_batch)\n    pred = tf.nn.softmax(pred)\n    imgs.append(image_batch)\n    true_labels.append(label_batch)\n    predictions.append(np.argmax(pred.numpy(),axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:26:15.424014Z","iopub.execute_input":"2022-03-27T14:26:15.424279Z","iopub.status.idle":"2022-03-27T14:27:09.121074Z","shell.execute_reply.started":"2022-03-27T14:26:15.424246Z","shell.execute_reply":"2022-03-27T14:27:09.120316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = [item for sublist in imgs for item in sublist]\npredictions = [item for sublist in predictions for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:09.122589Z","iopub.execute_input":"2022-03-27T14:27:09.122841Z","iopub.status.idle":"2022-03-27T14:27:09.130006Z","shell.execute_reply.started":"2022-03-27T14:27:09.122807Z","shell.execute_reply":"2022-03-27T14:27:09.12899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_sports = [INDEX_TO_CLASS[k] for k in predictions]\ntrue_labels_sports = [INDEX_TO_CLASS[k] for k in true_labels]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:09.13556Z","iopub.execute_input":"2022-03-27T14:27:09.136364Z","iopub.status.idle":"2022-03-27T14:27:09.147075Z","shell.execute_reply.started":"2022-03-27T14:27:09.136325Z","shell.execute_reply":"2022-03-27T14:27:09.14637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_pred, accuracy = model.evaluate(test_dataset)\nprint(\"Test loss :\", loss_pred)\nprint(\"Test accuracy :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:09.148262Z","iopub.execute_input":"2022-03-27T14:27:09.148664Z","iopub.status.idle":"2022-03-27T14:27:19.399525Z","shell.execute_reply.started":"2022-03-27T14:27:09.148625Z","shell.execute_reply":"2022-03-27T14:27:19.39873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(true_labels_sports, predictions_sports, )\nplt.figure(figsize=(20, 10))\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=sorted(\n    set(true_labels_sports)), yticklabels=sorted(set(true_labels_sports)))\nplt.xlabel(\"True Labels\")\nplt.ylabel(\"Predicted Labels\")\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:19.400576Z","iopub.execute_input":"2022-03-27T14:27:19.401075Z","iopub.status.idle":"2022-03-27T14:27:21.107612Z","shell.execute_reply.started":"2022-03-27T14:27:19.401034Z","shell.execute_reply":"2022-03-27T14:27:21.106958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n \nprint(classification_report(true_labels_sports, predictions_sports, target_names=class_names))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.108942Z","iopub.execute_input":"2022-03-27T14:27:21.109414Z","iopub.status.idle":"2022-03-27T14:27:21.154464Z","shell.execute_reply.started":"2022-03-27T14:27:21.109379Z","shell.execute_reply":"2022-03-27T14:27:21.153757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Heatmap Visualization ","metadata":{}},{"cell_type":"markdown","source":"In general, a CNN is often qualify as a \"black box\" because people doesn't really understand what a model learn exactly during training or how i it takes a decision for a prediction. In order to more understand that, one interesting thing to do is to visualize what, for a given image, the model focuses on in order to predict the corresponding label. That is why in the following cells, we are going to plot what is called Heatmap.\n\nWe will implement the method, called Grad-CAM, that is introduced in the following research paper : https://arxiv.org/abs/1610.02391#:~:text=Our%20approach%20%2D%20Gradient%2Dweighted%20Class,image%20for%20predicting%20the%20concept.\n","metadata":{}},{"cell_type":"markdown","source":"#### Grad-CAM","metadata":{}},{"cell_type":"markdown","source":"The following cell implements the Grad-CAM method is inspired from the following tutorial: \nhttps://keras.io/examples/vision/grad_cam/\n\nWe adapt this to our model","metadata":{}},{"cell_type":"code","source":"# The Grad-CAM algorithm\ndef get_img_array(img_path, size=(224,224,3)):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(\n    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(base_model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n    return heatmap","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.15694Z","iopub.execute_input":"2022-03-27T14:27:21.157127Z","iopub.status.idle":"2022-03-27T14:27:21.16977Z","shell.execute_reply.started":"2022-03-27T14:27:21.157103Z","shell.execute_reply":"2022-03-27T14:27:21.169095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Heatmap for our model","metadata":{}},{"cell_type":"markdown","source":"Firstly, we have to get the last convolution layer of the pretrained model (e.g. <code>ResNet50</code> in our case) and the predicition layers and store them in two variables called respectively <code>last_conv_layer_name</code>, and <code>classifier_layer_names</code>","metadata":{}},{"cell_type":"code","source":"for layer in model.layers:\n    print(layer.name)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.172632Z","iopub.execute_input":"2022-03-27T14:27:21.172834Z","iopub.status.idle":"2022-03-27T14:27:21.183532Z","shell.execute_reply.started":"2022-03-27T14:27:21.172804Z","shell.execute_reply":"2022-03-27T14:27:21.182884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet50_model = model.layers[4]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.185219Z","iopub.execute_input":"2022-03-27T14:27:21.185624Z","iopub.status.idle":"2022-03-27T14:27:21.193999Z","shell.execute_reply.started":"2022-03-27T14:27:21.185591Z","shell.execute_reply":"2022-03-27T14:27:21.193025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in resnet50_model.layers:\n    print(layer.name)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.1954Z","iopub.execute_input":"2022-03-27T14:27:21.195645Z","iopub.status.idle":"2022-03-27T14:27:21.228711Z","shell.execute_reply.started":"2022-03-27T14:27:21.195613Z","shell.execute_reply":"2022-03-27T14:27:21.228077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_conv_layer_name = \"conv5_block3_out\"\nclassifier_layer_names = [layer.name for layer in model.layers][5:]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.231216Z","iopub.execute_input":"2022-03-27T14:27:21.231392Z","iopub.status.idle":"2022-03-27T14:27:21.237183Z","shell.execute_reply.started":"2022-03-27T14:27:21.231371Z","shell.execute_reply":"2022-03-27T14:27:21.236384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now plot the heatmap of images from the testing dataset and see, for each class, what the model is focusing on the make a prediction","metadata":{}},{"cell_type":"code","source":"for sport in class_names:\n    sport_folder = os.path.join(TEST_DATA, sport)\n    sport_list = os.listdir(sport_folder)\n    \n    #img1_path = os.path.join(sport_folder, sport_list[random.randint(0, len(sport_list)-1)])\n    #img2_path = os.path.join(sport_folder, sport_list[random.randint(0, len(sport_list)-1)])\n    \n    img1_path = os.path.join(sport_folder, sport_list[0])\n    img2_path = os.path.join(sport_folder, sport_list[1])\n    \n    img1 = tf.keras.utils.load_img(img1_path, target_size=(224,224,3))\n    img1 = keras.preprocessing.image.img_to_array(img1)\n    img2 = tf.keras.utils.load_img(img2_path, target_size=(224,224,3))\n    img2 = keras.preprocessing.image.img_to_array(img2)\n  \n    heatmap1 = make_gradcam_heatmap(tf.expand_dims(img1,axis=0), resnet50_model, model, last_conv_layer_name, classifier_layer_names)\n    heatmap2 = make_gradcam_heatmap(tf.expand_dims(img2,axis=0), resnet50_model, model, last_conv_layer_name, classifier_layer_names)\n    \n    # We rescale heatmap to a range 0-255\n    heatmap1 = np.uint8(255 * heatmap1)\n    heatmap2 = np.uint8(255 * heatmap2)\n    \n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap1 = jet_colors[heatmap1]\n    jet_heatmap2 = jet_colors[heatmap2]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap1 = keras.preprocessing.image.array_to_img(jet_heatmap1)\n    jet_heatmap1 = jet_heatmap1.resize((224, 224))\n    jet_heatmap1 = keras.preprocessing.image.img_to_array(jet_heatmap1)\n    \n    jet_heatmap2 = keras.preprocessing.image.array_to_img(jet_heatmap2)\n    jet_heatmap2 = jet_heatmap2.resize((224, 224))\n    jet_heatmap2 = keras.preprocessing.image.img_to_array(jet_heatmap2)\n\n    # Superimpose the heatmap on original image\n    superimposed_img1 = jet_heatmap1 * 1.5 + img1\n    superimposed_img1 = keras.preprocessing.image.array_to_img(superimposed_img1)\n    \n    superimposed_img2 = jet_heatmap2 * 1.5 + img2\n    superimposed_img2 = keras.preprocessing.image.array_to_img(superimposed_img2)\n    \n    plt.figure(figsize=(8, 8))\n    \n    plt.suptitle('True Label : ' +  str(sport))\n    \n    plt.subplot(2, 2, 1)\n    plt.axis('off')\n    plt.imshow(superimposed_img1)\n    \n    plt.subplot(2, 2, 2)\n    plt.axis('off')\n    plt.imshow(superimposed_img2)\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:21.23945Z","iopub.execute_input":"2022-03-27T14:27:21.239808Z","iopub.status.idle":"2022-03-27T14:27:33.652553Z","shell.execute_reply.started":"2022-03-27T14:27:21.239772Z","shell.execute_reply":"2022-03-27T14:27:33.648613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"markdown","source":"In order to increase the performance even further, we will train (or \"fine-tune\") the weights of the top layers of the <code>ResNet50</code> pre-trained model alongside the training of the classifier we added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.","metadata":{}},{"cell_type":"code","source":"base_model.trainable = True\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:33.653859Z","iopub.execute_input":"2022-03-27T14:27:33.654578Z","iopub.status.idle":"2022-03-27T14:27:33.684493Z","shell.execute_reply.started":"2022-03-27T14:27:33.654538Z","shell.execute_reply":"2022-03-27T14:27:33.683812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cell above, we have unfreeze the weight. Consequently, as we are now training a much larger model and that we want to readapt the pretrained weights, it is important to use a lower learning rate for the optimizer. Otherwise, our model could overfit very quickly.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),  \n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nfine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_ft = model.fit(train_dataset,\n                        epochs=total_epochs,\n                        validation_data=val_dataset,\n                        initial_epoch=history_tl.epoch[-1])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:27:33.685698Z","iopub.execute_input":"2022-03-27T14:27:33.685925Z","iopub.status.idle":"2022-03-27T14:40:18.11294Z","shell.execute_reply.started":"2022-03-27T14:27:33.685895Z","shell.execute_reply":"2022-03-27T14:40:18.112197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss and Accuracy","metadata":{}},{"cell_type":"markdown","source":"We can now look at learning curves of the training and validation accuracy/loss","metadata":{}},{"cell_type":"code","source":"acc += history_ft.history['sparse_categorical_accuracy']\nval_acc += history_ft.history['val_sparse_categorical_accuracy']\n\nloss += history_ft.history['loss']\nval_loss += history_ft.history['val_loss']","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.114641Z","iopub.execute_input":"2022-03-27T14:40:18.114912Z","iopub.status.idle":"2022-03-27T14:40:18.85607Z","shell.execute_reply.started":"2022-03-27T14:40:18.114876Z","shell.execute_reply":"2022-03-27T14:40:18.854985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.857163Z","iopub.status.idle":"2022-03-27T14:40:18.85806Z","shell.execute_reply.started":"2022-03-27T14:40:18.857824Z","shell.execute_reply":"2022-03-27T14:40:18.857849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction of new samples and analysis","metadata":{}},{"cell_type":"code","source":"predictions = []\ntrue_labels = []\nimgs = []\n\nfor X, y in test_dataset :\n    image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n    pred = model.predict_on_batch(image_batch)\n    pred = tf.nn.softmax(pred)\n    imgs.append(image_batch)\n    true_labels.append(label_batch)\n    predictions.append(np.argmax(pred.numpy(),axis=1))\n    \npredictions = [item for sublist in predictions for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n\npredictions_sports = [INDEX_TO_CLASS[k] for k in predictions]\ntrue_labels_sports = [INDEX_TO_CLASS[k] for k in true_labels]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.859209Z","iopub.status.idle":"2022-03-27T14:40:18.860038Z","shell.execute_reply.started":"2022-03-27T14:40:18.859801Z","shell.execute_reply":"2022-03-27T14:40:18.859826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(true_labels_sports, predictions_sports, )\nplt.figure(figsize=(20, 10))\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=sorted(\n    set(true_labels_sports)), yticklabels=sorted(set(true_labels_sports)))\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.861114Z","iopub.status.idle":"2022-03-27T14:40:18.861924Z","shell.execute_reply.started":"2022-03-27T14:40:18.861692Z","shell.execute_reply":"2022-03-27T14:40:18.861716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" print(classification_report(true_labels_sports, predictions_sports, target_names=class_names))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.862968Z","iopub.status.idle":"2022-03-27T14:40:18.863781Z","shell.execute_reply.started":"2022-03-27T14:40:18.863549Z","shell.execute_reply":"2022-03-27T14:40:18.863573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_pred, accuracy = model.evaluate(test_dataset)\nprint(\"Test loss :\", loss_pred)\nprint(\"Test accuracy :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.864958Z","iopub.status.idle":"2022-03-27T14:40:18.865517Z","shell.execute_reply.started":"2022-03-27T14:40:18.86526Z","shell.execute_reply":"2022-03-27T14:40:18.865282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap Visualization ","metadata":{}},{"cell_type":"markdown","source":"Let see the heatmaps associated to this model","metadata":{}},{"cell_type":"code","source":"for sport in class_names:\n    sport_folder = os.path.join(TEST_DATA, sport)\n    sport_list = os.listdir(sport_folder)\n    \n    #img1_path = os.path.join(sport_folder, sport_list[random.randint(0, len(sport_list)-1)])\n    #img2_path = os.path.join(sport_folder, sport_list[random.randint(0, len(sport_list)-1)])\n    \n    img1_path = os.path.join(sport_folder, sport_list[0])\n    img2_path = os.path.join(sport_folder, sport_list[1])\n    \n    img1 = tf.keras.utils.load_img(img1_path, target_size=(224,224,3))\n    img1 = keras.preprocessing.image.img_to_array(img1)\n    img2 = tf.keras.utils.load_img(img2_path, target_size=(224,224,3))\n    img2 = keras.preprocessing.image.img_to_array(img2)\n  \n    heatmap1 = make_gradcam_heatmap(tf.expand_dims(img1,axis=0), resnet50_model, model, last_conv_layer_name, classifier_layer_names)\n    heatmap2 = make_gradcam_heatmap(tf.expand_dims(img2,axis=0), resnet50_model, model, last_conv_layer_name, classifier_layer_names)\n    \n    # We rescale heatmap to a range 0-255\n    heatmap1 = np.uint8(255 * heatmap1)\n    heatmap2 = np.uint8(255 * heatmap2)\n    \n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap1 = jet_colors[heatmap1]\n    jet_heatmap2 = jet_colors[heatmap2]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap1 = keras.preprocessing.image.array_to_img(jet_heatmap1)\n    jet_heatmap1 = jet_heatmap1.resize((224, 224))\n    jet_heatmap1 = keras.preprocessing.image.img_to_array(jet_heatmap1)\n    \n    jet_heatmap2 = keras.preprocessing.image.array_to_img(jet_heatmap2)\n    jet_heatmap2 = jet_heatmap2.resize((224, 224))\n    jet_heatmap2 = keras.preprocessing.image.img_to_array(jet_heatmap2)\n\n    # Superimpose the heatmap on original image\n    superimposed_img1 = jet_heatmap1 * 1.5 + img1\n    superimposed_img1 = keras.preprocessing.image.array_to_img(superimposed_img1)\n    \n    superimposed_img2 = jet_heatmap2 * 1.5 + img2\n    superimposed_img2 = keras.preprocessing.image.array_to_img(superimposed_img2)\n    \n    plt.figure(figsize=(8, 8))\n    \n    plt.suptitle('True Label : ' +  str(sport))\n    \n    plt.subplot(2, 2, 1)\n    plt.axis('off')\n    plt.imshow(superimposed_img1)\n    \n    plt.subplot(2, 2, 2)\n    plt.axis('off')\n    plt.imshow(superimposed_img2)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T14:40:18.866993Z","iopub.status.idle":"2022-03-27T14:40:18.867409Z","shell.execute_reply.started":"2022-03-27T14:40:18.867199Z","shell.execute_reply":"2022-03-27T14:40:18.86722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remarks & Improvements","metadata":{}},{"cell_type":"markdown","source":"- So far, we can see that after Transfer Learning, the performance of the model is reasonable and that after Fine Tuning, it has improved a little bit. In fact, Fine Tuning helps just for confusion between some classes.\n- In Data-Exploration, we have seen that there are some classes that have images quite similar between each others, such as tennis and badminton or wwe and wrestling. Consequently, it might be more interesting to introspect the quality of the images in the training set to check for ones that are too ambiguous and should be removed from it.\n- The impact of the optimizer is important. One thing that can be done is to tune the most appropriate learning rate before executing the real training of the model in order to have better results.","metadata":{}}]}