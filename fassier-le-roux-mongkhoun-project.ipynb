{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Sport Image Classification**","metadata":{}},{"cell_type":"markdown","source":"### Team Members :\n- FASSIER Thimoth√©\n- LE ROUX Thomas\n- MONGKHOUN Vincent\n\nIn this project, we are going to build a sport-image classifier. The model will take as an input an image and will return as an output the sport that is represented in it.","metadata":{}},{"cell_type":"markdown","source":"During this project, we have used some fonctions that were introduced during lecture labs about CNN and Computer Vision of this course.","metadata":{}},{"cell_type":"markdown","source":"This notebook of the project is divided into 4 parts :\n- **Data Exploration and Data Pre-processing**\n- **Transfer Learning with CNN features extraction**\n- **Transfer Learning with Fine Tuning**\n- **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"# Importation of packages","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\nimport keras\nfrom tensorflow.keras.models import Model\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport random\nimport torch.nn.functional as F\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport cv2\n\nimport os\nimport shutil","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-26T22:33:05.835384Z","iopub.execute_input":"2022-03-26T22:33:05.835704Z","iopub.status.idle":"2022-03-26T22:33:13.393244Z","shell.execute_reply.started":"2022-03-26T22:33:05.835622Z","shell.execute_reply":"2022-03-26T22:33:13.392481Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"## Test-data creation","metadata":{}},{"cell_type":"markdown","source":"The original dataset doesn't provide a folder where test data are located. Consequently, before beginning the data exploration and pre-precossing, we need to allocate some images as test data for the testing phase of our trained model","metadata":{}},{"cell_type":"code","source":"Kaggle = True\n\nif Kaggle : \n    DATASET_DIR = '../input/sports-image-dataset/sports-image-dataset'\nelse :\n    DATASET_DIR = './sports-image-dataset'\n\nDATA_DIR = os.path.join(DATASET_DIR, 'data')\nsports = os.listdir(DATA_DIR)\n\nprint('The sports that our model will classify are the following :')\nprint(sports)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:13.395807Z","iopub.execute_input":"2022-03-26T22:33:13.397069Z","iopub.status.idle":"2022-03-26T22:33:13.416345Z","shell.execute_reply.started":"2022-03-26T22:33:13.397028Z","shell.execute_reply":"2022-03-26T22:33:13.415659Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Let create a test data subfolder to store the data that will be used for the testing phase. We randomly select the data from each classes ","metadata":{}},{"cell_type":"code","source":"# Creation of the test subfolder and the dataset\n\nn_validation = 100\n\ntest_folder = os.path.join(DATASET_DIR, 'test_data')\nif not os.path.exists(test_folder):\n    os.mkdir(test_folder)\n    for class_name in sports:\n        train_subfolder = os.path.join(DATA_DIR, class_name)\n        test_subfolder = os.path.join(test_folder, class_name)\n        print(\"Populating %s...\" % test_subfolder)\n        os.mkdir(test_subfolder)\n        images_filenames = sorted(os.listdir(train_subfolder))\n        for image_filename in images_filenames[-n_validation:]:\n            shutil.move(os.path.join(train_subfolder, image_filename),\n                        test_subfolder)\n        print(\"Moved %d images\" % len(os.listdir(test_subfolder)))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:13.417752Z","iopub.execute_input":"2022-03-26T22:33:13.418019Z","iopub.status.idle":"2022-03-26T22:33:13.426644Z","shell.execute_reply.started":"2022-03-26T22:33:13.417983Z","shell.execute_reply":"2022-03-26T22:33:13.425627Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Creation of the tensorflow-datasets","metadata":{}},{"cell_type":"markdown","source":"As we have a set of images from our dataset that are filed into class-specific folders, we will use the method <code>tf.keras.utils.image_dataset_from_directory</code> to automatically process the data directly from the train subfolder and also to generate similar labeled dataset objects. \n\nWe divide the data into 80% for the train dataset and 20% for the validation dataset.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nimg_height = 224\nimg_width = 224","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:13.428995Z","iopub.execute_input":"2022-03-26T22:33:13.429820Z","iopub.status.idle":"2022-03-26T22:33:13.433676Z","shell.execute_reply.started":"2022-03-26T22:33:13.429784Z","shell.execute_reply":"2022-03-26T22:33:13.432797Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.keras.utils.image_dataset_from_directory(\n  DATA_DIR,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n  color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:13.435154Z","iopub.execute_input":"2022-03-26T22:33:13.436005Z","iopub.status.idle":"2022-03-26T22:33:19.864594Z","shell.execute_reply.started":"2022-03-26T22:33:13.435966Z","shell.execute_reply":"2022-03-26T22:33:19.863801Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"val_dataset = tf.keras.utils.image_dataset_from_directory(\n  DATA_DIR,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n  color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:19.865956Z","iopub.execute_input":"2022-03-26T22:33:19.866378Z","iopub.status.idle":"2022-03-26T22:33:20.646716Z","shell.execute_reply.started":"2022-03-26T22:33:19.866337Z","shell.execute_reply":"2022-03-26T22:33:20.646013Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can check if everything is good by looking at the shape of one batch","metadata":{}},{"cell_type":"code","source":"# Check how the images are arranged within a batch \nfor image_batch, labels_batch in train_dataset:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:20.647924Z","iopub.execute_input":"2022-03-26T22:33:20.648173Z","iopub.status.idle":"2022-03-26T22:33:21.940769Z","shell.execute_reply.started":"2022-03-26T22:33:20.648138Z","shell.execute_reply":"2022-03-26T22:33:21.940073Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Everything seems to be good as a batch of size 32 and the images are correctly converted into (img_height, img_width, nb_channels) format.","metadata":{}},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"class_names = train_dataset.class_names\nnb_sports = len(class_names)\nprint('We will classify the folliwing sports : ' + str(class_names))\nprint('There is a total of ' + str(nb_sports) + ' sports')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:21.941918Z","iopub.execute_input":"2022-03-26T22:33:21.942169Z","iopub.status.idle":"2022-03-26T22:33:21.948108Z","shell.execute_reply.started":"2022-03-26T22:33:21.942122Z","shell.execute_reply":"2022-03-26T22:33:21.947443Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"In order to quickly switch between the name of the sport and the corresponding label, we define the following dictionay :","metadata":{}},{"cell_type":"code","source":"# Create a quick path between label and sport\nINDEX_TO_CLASS = {k: v for k, v in enumerate(class_names)}\nINDEX_TO_CLASS","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:21.949311Z","iopub.execute_input":"2022-03-26T22:33:21.949819Z","iopub.status.idle":"2022-03-26T22:33:21.962865Z","shell.execute_reply.started":"2022-03-26T22:33:21.949785Z","shell.execute_reply":"2022-03-26T22:33:21.962032Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We can now plot one image of each class to see what it's look like","metadata":{}},{"cell_type":"code","source":"# Plot one image per class\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n    for i in range(22):\n        ax = plt.subplot(5, 5, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:21.966613Z","iopub.execute_input":"2022-03-26T22:33:21.966835Z","iopub.status.idle":"2022-03-26T22:33:23.587673Z","shell.execute_reply.started":"2022-03-26T22:33:21.966811Z","shell.execute_reply":"2022-03-26T22:33:23.584628Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Let's now see what the images from the same class looks like. We first define a function that plot the different images from the same class","metadata":{}},{"cell_type":"code","source":"def plot_classes(random_sport):\n    '''Plot images from the same class'''\n\n    n_rows = 3\n    n_cols = 4\n    sport_folder = os.path.join(DATA_DIR, random_sport)\n    list_img = os.listdir(sport_folder)\n    \n    for row in range(n_rows):\n        for col in range(n_cols):\n            index = n_cols * row + col\n            plt.subplot(n_rows, n_cols, index+1)\n\n            # Pick a random images to plot\n            for i in range(8):\n                x = random.randint(0, len(list_img) - 1)\n                image_ = plt.imread(os.path.join(sport_folder, list_img[x]))\n                # Display the image\n                plt.imshow(image_, cmap='binary', interpolation='nearest')\n                plt.title(random_sport)\n                plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:23.588946Z","iopub.execute_input":"2022-03-26T22:33:23.589203Z","iopub.status.idle":"2022-03-26T22:33:23.611965Z","shell.execute_reply.started":"2022-03-26T22:33:23.589170Z","shell.execute_reply":"2022-03-26T22:33:23.610583Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"random_sport = INDEX_TO_CLASS[random.randint(0, nb_sports)]\nprint(f'The selected sport to display various images is {random_sport.upper()}')\n\nplot_classes(random_sport)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:23.613374Z","iopub.execute_input":"2022-03-26T22:33:23.613647Z","iopub.status.idle":"2022-03-26T22:33:25.822998Z","shell.execute_reply.started":"2022-03-26T22:33:23.613597Z","shell.execute_reply":"2022-03-26T22:33:25.822359Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"random_sport = INDEX_TO_CLASS[random.randint(0, nb_sports)]\nprint(f'The selected sport to display various images is {random_sport.upper()}')\n\nplot_classes(random_sport)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:25.824523Z","iopub.execute_input":"2022-03-26T22:33:25.825407Z","iopub.status.idle":"2022-03-26T22:33:28.675106Z","shell.execute_reply.started":"2022-03-26T22:33:25.825362Z","shell.execute_reply":"2022-03-26T22:33:28.674424Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"random_sport = INDEX_TO_CLASS[random.randint(0, nb_sports)]\nprint(f'The selected sport to display various images is {random_sport.upper()}')\n\nplot_classes(random_sport)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:28.676487Z","iopub.execute_input":"2022-03-26T22:33:28.676762Z","iopub.status.idle":"2022-03-26T22:33:30.654369Z","shell.execute_reply.started":"2022-03-26T22:33:28.676726Z","shell.execute_reply":"2022-03-26T22:33:30.653708Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We remark that within the same class, images are quiet different in term of orientation, color, zoom range or even light levels. There are even images of the equipment only, such as for hockey, basketball, badminton or football. The model should be agnostic or robust to these variations.  The idea of using data augmention becomes pertinent, and it's all the more the case as the train dataset is quite small for this type of case (Image Classification).\n\n\n**Remark :** the images are not in the same size here but there will have the same during the training (cf Creation of Tensorflow Dataset section)","metadata":{}},{"cell_type":"markdown","source":"Let's check the number of images per each classes","metadata":{}},{"cell_type":"code","source":"effectifs = []\nfor sport in class_names:\n    effectif = os.listdir(os.path.join(DATA_DIR, sport))\n    effectifs.append(len(effectif))\n\nplt.figure(figsize=(25, 10))\nsns.barplot(x=class_names, y=effectifs, palette='viridis')\nplt.title('Images per Sports',)\nplt.ylabel('Number of images')\nplt.xlabel('Sports Name')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:30.656364Z","iopub.execute_input":"2022-03-26T22:33:30.656829Z","iopub.status.idle":"2022-03-26T22:33:31.057669Z","shell.execute_reply.started":"2022-03-26T22:33:30.656789Z","shell.execute_reply":"2022-03-26T22:33:31.056971Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The 2 sports that are the most represented in the dataset are firstly Badminton and secondly Football. On the contrary, the ones that are the less represented are Basketball, Kabaddi and Chess. ","metadata":{}},{"cell_type":"markdown","source":"## Data-Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We first configure the performance of the dataset.\n\nThe <code>Dataset.cache</code> keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training our model.\n\nThe <code>Dataset.prefetch</code> overlaps data preprocessing and model execution while training.","metadata":{}},{"cell_type":"code","source":"train_dataset = train_dataset.cache().prefetch(buffer_size=10)\nval_dataset = val_dataset.cache().prefetch(buffer_size=10)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:31.058878Z","iopub.execute_input":"2022-03-26T22:33:31.059631Z","iopub.status.idle":"2022-03-26T22:33:31.066793Z","shell.execute_reply.started":"2022-03-26T22:33:31.059594Z","shell.execute_reply":"2022-03-26T22:33:31.066143Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We can noww configure the different data augmentation process that we will use during the training. Regarding the different variation of images within the same class, we have decided to do : \n- Horizontal and Vertical <code>RandomFlip</code>\n- <code>RandomRotation</code> ","metadata":{}},{"cell_type":"code","source":"# Create the random data augmentation \ndata_augmentation = tf.keras.Sequential(\n    [tf.keras.layers.RandomFlip(\"horizontal\"),\n     tf.keras.layers.RandomRotation(0.1),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:31.068033Z","iopub.execute_input":"2022-03-26T22:33:31.068373Z","iopub.status.idle":"2022-03-26T22:33:31.096187Z","shell.execute_reply.started":"2022-03-26T22:33:31.068337Z","shell.execute_reply":"2022-03-26T22:33:31.095595Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We can test if everithing works well for one image","metadata":{}},{"cell_type":"code","source":"for images, labels in train_dataset.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = images[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(\n            tf.expand_dims(first_image, 0), training=True)\n        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n        plt.title(INDEX_TO_CLASS[int(labels[0])])\n        plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:31.097300Z","iopub.execute_input":"2022-03-26T22:33:31.097515Z","iopub.status.idle":"2022-03-26T22:33:32.522978Z","shell.execute_reply.started":"2022-03-26T22:33:31.097485Z","shell.execute_reply":"2022-03-26T22:33:32.522383Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning with CNN Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"In this section, we will implement a Transfer Learning model. We will use a pretrained CNN in order to process feature extraction. The aim is to, from a previous network, extract meaningful features from new samples. We will finally add a new model on top of this pretrained model so that we can repurpose the feature maps learned previously for the dataset.","metadata":{}},{"cell_type":"markdown","source":"## The model","metadata":{}},{"cell_type":"markdown","source":"The CNN that we are using is the <code>ResNet50</code> model, which is a quite good model that present a good balance between performance and time of computation\n\nFor feature extraction, we specify <code>the include_top=False</code> argument, so that when we load the network, it doesn't include the classification layers at the top. ","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.resnet50.ResNet50(\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n    input_shape=(224, 224, 3),\n    include_top=False,\n)  ","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:32.524351Z","iopub.execute_input":"2022-03-26T22:33:32.524786Z","iopub.status.idle":"2022-03-26T22:33:34.355792Z","shell.execute_reply.started":"2022-03-26T22:33:32.524750Z","shell.execute_reply":"2022-03-26T22:33:34.354837Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"We want to prevent the weights in a given layer of the <code>based_model</code> from being updated during training, so we freeze it : ","metadata":{}},{"cell_type":"code","source":"# Freeze the base_model\nbase_model.trainable = False\n\n# Let's see at the base model architecture\nbase_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:34.357021Z","iopub.execute_input":"2022-03-26T22:33:34.357257Z","iopub.status.idle":"2022-03-26T22:33:34.441677Z","shell.execute_reply.started":"2022-03-26T22:33:34.357224Z","shell.execute_reply":"2022-03-26T22:33:34.441020Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Once we have freezed the original top layer of the <code>Xception</code>, we can add our own classifier on the top of the model so it will be ready to be train with our own data.\n\nIn order to be fed to the CNN feature extractor, images have to be rescale. Then, we add a <code>Rescaling</code> layer to scale input values (initially in the <code>[0, 255]</code> range) to the <code>[-1, 1]</code> range. We also add a <code>Dropout</code> for regularization. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import preprocess_input\n\n# Create new model on top\ninputs = tf.keras.Input(shape=(224, 224, 3))\n\n# Data augmentation\nx = data_augmentation(inputs)\n\n# Pre-processing the data in order to be fed to the model\nx = preprocess_input(x)\n\nx = base_model(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\n# Regularization\nx = keras.layers.Dropout(0.2)(x)\n\noutputs = keras.layers.Dense(22, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs, outputs)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:34.442966Z","iopub.execute_input":"2022-03-26T22:33:34.443203Z","iopub.status.idle":"2022-03-26T22:33:34.991804Z","shell.execute_reply.started":"2022-03-26T22:33:34.443171Z","shell.execute_reply":"2022-03-26T22:33:34.991111Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"We can now proceed to the training. We have decided to choose the following parameters : \n- <code>SGD</code> optimizer with default value of learning rate\n- <code>SparseCategoricalCrossentropy</code> loss as we didn't onehot encoded our labels and that we 22 labels and not 2 (otherwise it we be <code>CategoricalCrossentropy</code>)\n- <code>SparseCategoricalAccuracy</code> metric ","metadata":{}},{"cell_type":"code","source":"# Definition of the optimization parameters and of the metric\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.SGD(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:34.993241Z","iopub.execute_input":"2022-03-26T22:33:34.993482Z","iopub.status.idle":"2022-03-26T22:33:35.012549Z","shell.execute_reply.started":"2022-03-26T22:33:34.993448Z","shell.execute_reply":"2022-03-26T22:33:35.011887Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Training\n\ninitial_epochs = 20\nhistory_tl = model.fit(train_dataset,\n                       epochs=initial_epochs,\n                       validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:33:35.013576Z","iopub.execute_input":"2022-03-26T22:33:35.013827Z","iopub.status.idle":"2022-03-26T22:45:14.831996Z","shell.execute_reply.started":"2022-03-26T22:33:35.013793Z","shell.execute_reply":"2022-03-26T22:45:14.831288Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Perfomance of the model","metadata":{}},{"cell_type":"markdown","source":"### Loss and Accuracy","metadata":{}},{"cell_type":"markdown","source":"We can now look at learning curves of the training and validation accuracy/loss","metadata":{}},{"cell_type":"code","source":"acc = history_tl.history['sparse_categorical_accuracy']\nval_acc = history_tl.history['val_sparse_categorical_accuracy']\n\nloss = history_tl.history['loss']\nval_loss = history_tl.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:45:14.833437Z","iopub.execute_input":"2022-03-26T22:45:14.833941Z","iopub.status.idle":"2022-03-26T22:45:15.150051Z","shell.execute_reply.started":"2022-03-26T22:45:14.833904Z","shell.execute_reply":"2022-03-26T22:45:15.149404Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Prediction of new samples and analysis","metadata":{}},{"cell_type":"code","source":"TEST_DATA = os.path.join(DATASET_DIR, 'test_data')\n\ntest_dataset = tf.keras.utils.image_dataset_from_directory(\n    TEST_DATA,\n    image_size=(img_height, img_width),\n    color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:45:15.151405Z","iopub.execute_input":"2022-03-26T22:45:15.151882Z","iopub.status.idle":"2022-03-26T22:45:15.496859Z","shell.execute_reply.started":"2022-03-26T22:45:15.151844Z","shell.execute_reply":"2022-03-26T22:45:15.496128Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntrue_labels = []\n\nfor X, y in test_dataset :\n    image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n    pred = model.predict_on_batch(image_batch)\n    pred = tf.nn.softmax(pred)\n    true_labels.append(label_batch)\n    predictions.append(np.argmax(pred.numpy(),axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:45:15.498118Z","iopub.execute_input":"2022-03-26T22:45:15.498493Z","iopub.status.idle":"2022-03-26T22:46:16.421143Z","shell.execute_reply.started":"2022-03-26T22:45:15.498458Z","shell.execute_reply":"2022-03-26T22:46:16.420388Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"predictions = [item for sublist in predictions for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:16.422489Z","iopub.execute_input":"2022-03-26T22:46:16.422794Z","iopub.status.idle":"2022-03-26T22:46:16.428402Z","shell.execute_reply.started":"2022-03-26T22:46:16.422756Z","shell.execute_reply":"2022-03-26T22:46:16.427529Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"predictions_sports = [INDEX_TO_CLASS[k] for k in predictions]\ntrue_labels_sports = [INDEX_TO_CLASS[k] for k in true_labels]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:16.436927Z","iopub.execute_input":"2022-03-26T22:46:16.437456Z","iopub.status.idle":"2022-03-26T22:46:16.448606Z","shell.execute_reply.started":"2022-03-26T22:46:16.437427Z","shell.execute_reply":"2022-03-26T22:46:16.447887Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"loss_pred, accuracy = model.evaluate(test_dataset)\nprint(\"Test loss :\", loss_pred)\nprint(\"Test accuracy :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:16.449870Z","iopub.execute_input":"2022-03-26T22:46:16.450123Z","iopub.status.idle":"2022-03-26T22:46:24.340414Z","shell.execute_reply.started":"2022-03-26T22:46:16.450088Z","shell.execute_reply":"2022-03-26T22:46:24.339745Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(true_labels_sports, predictions_sports, )\nplt.figure(figsize=(20, 10))\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=sorted(\n    set(true_labels_sports)), yticklabels=sorted(set(true_labels_sports)))\nplt.xlabel(\"True Labels\")\nplt.ylabel(\"Predicted Labels\")\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:24.341643Z","iopub.execute_input":"2022-03-26T22:46:24.341968Z","iopub.status.idle":"2022-03-26T22:46:25.994137Z","shell.execute_reply.started":"2022-03-26T22:46:24.341930Z","shell.execute_reply":"2022-03-26T22:46:25.993462Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n \nprint(classification_report(true_labels_sports, predictions_sports, target_names=class_names))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:25.995576Z","iopub.execute_input":"2022-03-26T22:46:25.996047Z","iopub.status.idle":"2022-03-26T22:46:26.042350Z","shell.execute_reply.started":"2022-03-26T22:46:25.996011Z","shell.execute_reply":"2022-03-26T22:46:26.041574Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Heatmap Visualization ","metadata":{}},{"cell_type":"markdown","source":"Def of a haetmap\nTo visualize the heatmap, we will follow the tutorial that is provided in the following link :  https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/. We adapt this code in order to match with our","metadata":{}},{"cell_type":"code","source":"'''class GradCAM:\n    def __init__(self, model, classIdx, layerName):\n        # store the model, the class index used to measure the class\n        # activation map, and the layer to be used when visualizing\n        # the class activation map\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n    \n    def compute_heatmap(self, image, eps=1e-8):\n        # construct our gradient model by supplying (1) the inputs\n        # to our pre-trained model, (2) the output of the (presumably)\n        # final 4D layer in the network, and (3) the output of the\n        # softmax activations from the model\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer('resnet50').get_layer(self.layerName).output,self.model.output])\n        # record operations for automatic differentiation\n        with tf.GradientTape() as tape:\n            # cast the image tensor to a float-32 data type, pass the\n            # image through the gradient model, and grab the loss\n            # associated with the specific class index\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            \n        # use automatic differentiation to compute the gradients\n        grads = tape.gradient(loss, convOutputs)\n        # compute the guided gradients\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n        # the convolution and guided gradients have a batch dimension\n        # (which we don't need) so let's grab the volume itself and\n        # discard the batch\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n        \n        # compute the average of the gradient values, and using them\n        # as weights, compute the ponderation of the filters with\n        # respect to the weights\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n        \n        # grab the spatial dimensions of the input image and resize\n        # the output class activation map to match the input image\n        # dimensions\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n        # normalize the heatmap such that all values lie in the range\n        # [0, 1], scale the resulting values to the range [0, 255],\n        # and then convert to an unsigned 8-bit integer\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n        # return the resulting heatmap to the calling function\n        return heatmap\n    \n    def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_VIRIDIS):\n        # apply the supplied color map to the heatmap and then overlay the heatmap on the input image\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        # return a 2-tuple of the color mapped heatmap and the output, overlaid image\n        return (heatmap, output)'''","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:26.043811Z","iopub.execute_input":"2022-03-26T22:46:26.044060Z","iopub.status.idle":"2022-03-26T22:46:26.053101Z","shell.execute_reply.started":"2022-03-26T22:46:26.044025Z","shell.execute_reply":"2022-03-26T22:46:26.052296Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"'''from tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.applications import imagenet_utils\n\n# load the input image from disk (in Keras/TensorFlow format) and preprocess it\nsport = 'basketball'\nsport_path = os.path.join(DATASET_DIR, 'test_data', sport)\nsport_list = os.listdir(sport_path)\nimg_random = random.randint(0, len(sport_list))\n\n# load the original image from disk (in OpenCV format) and then\n# resize the image to its target dimensions\norig = cv2.imread(os.path.join(sport_path, sport_list[img_random]))\nresized = cv2.resize(orig, (224, 224))\n\n# load the input image from disk (in Keras/TensorFlow format) and\n# preprocess it\nimage = load_img(os.path.join(sport_path, sport_list[img_random]), target_size=(224, 224))\nimage = img_to_array(image)\nimage = np.expand_dims(image, axis=0)\nimage = imagenet_utils.preprocess_input(image)\n\n# use the network to make predictions on the input image and find\n# the class label index with the largest corresponding probability\npreds = model.predict(image)\npred = tf.nn.softmax(preds)\ni = np.argmax(preds[0])\n# decode the ImageNet predictions to obtain the human-readable label\nlabel = INDEX_TO_CLASS[i]\nprint(\"The true label is {}\".format(sport))\nprint(\"The predicted label is {}\".format(label))\n\n# initialize our gradient class activation map and build the heatmap\ncam = GradCAM(model=model, classIdx=i, layerName='conv5_block3_out')\nheatmap = cam.compute_heatmap(image)\n# resize the resulting heatmap to the original input image dimensions\n# and then overlay heatmap on top of the image\nheatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n(heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n\ncv2.imshow(\"Original\",orig)\ncv2.imshow(\"Heatmap\", heatmap)\ncv2.imshow(\"Overlay\", output)'''","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:26.059289Z","iopub.execute_input":"2022-03-26T22:46:26.059643Z","iopub.status.idle":"2022-03-26T22:46:26.066280Z","shell.execute_reply.started":"2022-03-26T22:46:26.059611Z","shell.execute_reply":"2022-03-26T22:46:26.065606Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"'''def get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer('resnet50').get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n'''","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:26.067402Z","iopub.execute_input":"2022-03-26T22:46:26.068224Z","iopub.status.idle":"2022-03-26T22:46:26.080981Z","shell.execute_reply.started":"2022-03-26T22:46:26.068145Z","shell.execute_reply":"2022-03-26T22:46:26.080212Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"'''from tensorflow.keras.applications.resnet50 import decode_predictions\n\nlast_conv_layer_name = 'conv5_block3_out'\n# Prepare image\nimg_array = preprocess_input(get_img_array('../input/sports-image-dataset/sports-image-dataset/test_data/football/00000758.jpg', size=(224, 224, 3)))\n\n# Remove last layer's softmax\nbase_model.layers[-1].activation = None\n\n# Print what the top predicted class is\npreds = model.predict(img_array)\npred = tf.nn.softmax(preds)\ni = np.argmax(pred[0])\nlabel = INDEX_TO_CLASS[i]\n\n# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()'''","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:26.082434Z","iopub.execute_input":"2022-03-26T22:46:26.083045Z","iopub.status.idle":"2022-03-26T22:46:26.092365Z","shell.execute_reply.started":"2022-03-26T22:46:26.083008Z","shell.execute_reply":"2022-03-26T22:46:26.091475Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"markdown","source":"In order to increase the performance even further, we will train (or \"fine-tune\") the weights of the top layers of the <code>Xception</code> pre-trained model alongside the training of the classifier we added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.","metadata":{}},{"cell_type":"code","source":"base_model.trainable = True\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:26.093898Z","iopub.execute_input":"2022-03-26T22:46:26.094475Z","iopub.status.idle":"2022-03-26T22:46:26.122141Z","shell.execute_reply.started":"2022-03-26T22:46:26.094438Z","shell.execute_reply":"2022-03-26T22:46:26.121449Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"In the cell above, we have unfreeze the weight. Consequently, as we are now training a much larger model and that we want to readapt the pretrained weights, it is important to use a lower learning rate for the optimizer. Otherwise, our model could overfit very quickly.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9),  \n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nfine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_ft = model.fit(train_dataset,\n                        epochs=total_epochs,\n                        validation_data=val_dataset,\n                        initial_epoch=history_tl.epoch[-1])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:46:26.123318Z","iopub.execute_input":"2022-03-26T22:46:26.123755Z","iopub.status.idle":"2022-03-26T23:00:32.269289Z","shell.execute_reply.started":"2022-03-26T22:46:26.123721Z","shell.execute_reply":"2022-03-26T23:00:32.268499Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"RQ: if the validation loss is much higher than the training loss, so you may get some overfitting.","metadata":{}},{"cell_type":"markdown","source":"## Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"acc += history_ft.history['sparse_categorical_accuracy']\nval_acc += history_ft.history['val_sparse_categorical_accuracy']\n\nprint(loss)\n\nloss += history_ft.history['loss']\nval_loss += history_ft.history['val_loss']","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:02:46.212046Z","iopub.execute_input":"2022-03-26T23:02:46.212340Z","iopub.status.idle":"2022-03-26T23:02:46.240416Z","shell.execute_reply.started":"2022-03-26T23:02:46.212306Z","shell.execute_reply":"2022-03-26T23:02:46.239491Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:05:03.136507Z","iopub.execute_input":"2022-03-26T23:05:03.137165Z","iopub.status.idle":"2022-03-26T23:05:03.516352Z","shell.execute_reply.started":"2022-03-26T23:05:03.137126Z","shell.execute_reply":"2022-03-26T23:05:03.515623Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## Prediction of new samples and analysis","metadata":{}},{"cell_type":"code","source":"TEST_DATA = os.path.join(DATASET_DIR, 'test_data')\n\ntest_dataset = tf.keras.utils.image_dataset_from_directory(\n    TEST_DATA,\n    image_size=(img_height, img_width),\n    color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:05:17.627860Z","iopub.execute_input":"2022-03-26T23:05:17.628423Z","iopub.status.idle":"2022-03-26T23:05:17.878915Z","shell.execute_reply.started":"2022-03-26T23:05:17.628385Z","shell.execute_reply":"2022-03-26T23:05:17.878121Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntrue_labels = []\n\nfor X, y in test_dataset :\n    image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n    pred = model.predict_on_batch(image_batch)\n    pred = tf.nn.softmax(pred)\n    true_labels.append(label_batch)\n    predictions.append(np.argmax(pred.numpy(),axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:05:18.717687Z","iopub.execute_input":"2022-03-26T23:05:18.718152Z","iopub.status.idle":"2022-03-26T23:06:11.799833Z","shell.execute_reply.started":"2022-03-26T23:05:18.718110Z","shell.execute_reply":"2022-03-26T23:06:11.799043Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"predictions = [item for sublist in predictions for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:06:11.801580Z","iopub.execute_input":"2022-03-26T23:06:11.801853Z","iopub.status.idle":"2022-03-26T23:06:11.815520Z","shell.execute_reply.started":"2022-03-26T23:06:11.801818Z","shell.execute_reply":"2022-03-26T23:06:11.814821Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"predictions_sports = [INDEX_TO_CLASS[k] for k in predictions]\ntrue_labels_sports = [INDEX_TO_CLASS[k] for k in true_labels]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:06:11.817545Z","iopub.execute_input":"2022-03-26T23:06:11.818183Z","iopub.status.idle":"2022-03-26T23:06:11.830961Z","shell.execute_reply.started":"2022-03-26T23:06:11.818146Z","shell.execute_reply":"2022-03-26T23:06:11.830233Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(true_labels_sports, predictions_sports, )\nplt.figure(figsize=(20, 10))\nsns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=sorted(\n    set(true_labels_sports)), yticklabels=sorted(set(true_labels_sports)))\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:06:11.832424Z","iopub.execute_input":"2022-03-26T23:06:11.832676Z","iopub.status.idle":"2022-03-26T23:06:13.481238Z","shell.execute_reply.started":"2022-03-26T23:06:11.832643Z","shell.execute_reply":"2022-03-26T23:06:13.480590Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n \nprint(classification_report(true_labels_sports, predictions_sports, target_names=class_names))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:08:01.783371Z","iopub.execute_input":"2022-03-26T23:08:01.783844Z","iopub.status.idle":"2022-03-26T23:08:01.826475Z","shell.execute_reply.started":"2022-03-26T23:08:01.783808Z","shell.execute_reply":"2022-03-26T23:08:01.825766Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"loss_pred, accuracy = model.evaluate(test_dataset)\nprint(\"Test loss :\", loss_pred)\nprint(\"Test accuracy :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:08:06.397867Z","iopub.execute_input":"2022-03-26T23:08:06.398453Z","iopub.status.idle":"2022-03-26T23:08:16.664384Z","shell.execute_reply.started":"2022-03-26T23:08:06.398406Z","shell.execute_reply":"2022-03-26T23:08:16.663619Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"## Most confident mistakes","metadata":{}},{"cell_type":"markdown","source":"## Heatmap Visualization ","metadata":{}},{"cell_type":"code","source":"last_conv_layer_name = \"conv5_block3_out\"\n\nimage_ = plt.imread('../input/sports-image-dataset/sports-image-dataset/test_data/football/00000764.JPG')\nplt.imshow(image_, cmap='binary', interpolation='nearest')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:08:29.762895Z","iopub.execute_input":"2022-03-26T23:08:29.763146Z","iopub.status.idle":"2022-03-26T23:08:29.919658Z","shell.execute_reply.started":"2022-03-26T23:08:29.763119Z","shell.execute_reply":"2022-03-26T23:08:29.918997Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer('resnet50').get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:31:26.911830Z","iopub.execute_input":"2022-03-26T23:31:26.912483Z","iopub.status.idle":"2022-03-26T23:31:26.925856Z","shell.execute_reply.started":"2022-03-26T23:31:26.912434Z","shell.execute_reply":"2022-03-26T23:31:26.925124Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import decode_predictions\n\nlast_conv_layer_name = 'conv5_block3_out'\n# Prepare image\nimg_array = preprocess_input(get_img_array('../input/sports-image-dataset/sports-image-dataset/test_data/football/00000758.jpg', size=(224, 224, 3)))\n# Remove last layer's softmax\nmodel.layers[-1].activation = None\n\n# Print what the top predicted class is\npreds = model.predict_on_batch(image_batch)\npred = tf.nn.softmax(preds)\ni = np.argmax(pred[0])\nlabel = INDEX_TO_CLASS[i]\n\n# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:31:31.591197Z","iopub.execute_input":"2022-03-26T23:31:31.591715Z","iopub.status.idle":"2022-03-26T23:31:31.722961Z","shell.execute_reply.started":"2022-03-26T23:31:31.591678Z","shell.execute_reply":"2022-03-26T23:31:31.721578Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class GradCAM:\n    def __init__(self, model, classIdx, layerName):\n        # store the model, the class index used to measure the class\n        # activation map, and the layer to be used when visualizing\n        # the class activation map\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n    \n    def compute_heatmap(self, image, eps=1e-8):\n        # construct our gradient model by supplying (1) the inputs\n        # to our pre-trained model, (2) the output of the (presumably)\n        # final 4D layer in the network, and (3) the output of the\n        # softmax activations from the model\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer('resnet50').get_layer(self.layerName).output,self.model.output])\n        # record operations for automatic differentiation\n        with tf.GradientTape() as tape:\n            # cast the image tensor to a float-32 data type, pass the\n            # image through the gradient model, and grab the loss\n            # associated with the specific class index\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            \n        # use automatic differentiation to compute the gradients\n        grads = tape.gradient(loss, convOutputs)\n        # compute the guided gradients\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n        # the convolution and guided gradients have a batch dimension\n        # (which we don't need) so let's grab the volume itself and\n        # discard the batch\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n        \n        # compute the average of the gradient values, and using them\n        # as weights, compute the ponderation of the filters with\n        # respect to the weights\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n        \n        # grab the spatial dimensions of the input image and resize\n        # the output class activation map to match the input image\n        # dimensions\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n        # normalize the heatmap such that all values lie in the range\n        # [0, 1], scale the resulting values to the range [0, 255],\n        # and then convert to an unsigned 8-bit integer\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n        # return the resulting heatmap to the calling function\n        return heatmap\n    \n    def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_VIRIDIS):\n        # apply the supplied color map to the heatmap and then overlay the heatmap on the input image\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        # return a 2-tuple of the color mapped heatmap and the output, overlaid image\n        return (heatmap, output)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:32:02.834680Z","iopub.execute_input":"2022-03-26T23:32:02.835244Z","iopub.status.idle":"2022-03-26T23:32:02.850751Z","shell.execute_reply.started":"2022-03-26T23:32:02.835203Z","shell.execute_reply":"2022-03-26T23:32:02.849423Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.applications import imagenet_utils\n\n# load the input image from disk (in Keras/TensorFlow format) and preprocess it\nsport = 'basketball'\nsport_path = os.path.join(DATASET_DIR, 'test_data', sport)\nsport_list = os.listdir(sport_path)\nimg_random = random.randint(0, len(sport_list))\n\n# load the original image from disk (in OpenCV format) and then\n# resize the image to its target dimensions\norig = cv2.imread(os.path.join(sport_path, sport_list[img_random]))\nresized = cv2.resize(orig, (224, 224))\n\n# load the input image from disk (in Keras/TensorFlow format) and\n# preprocess it\nimage = load_img(os.path.join(sport_path, sport_list[img_random]), target_size=(224, 224))\nimage = img_to_array(image)\nimage = np.expand_dims(image, axis=0)\nimage = imagenet_utils.preprocess_input(image)\n\n# use the network to make predictions on the input image and find\n# the class label index with the largest corresponding probability\npreds = model.predict_on_batch(image)\npred = tf.nn.softmax(preds)\ni = np.argmax(preds[0])\n# decode the ImageNet predictions to obtain the human-readable label\nlabel = INDEX_TO_CLASS[i]\nprint(\"The true label is {}\".format(sport))\nprint(\"The predicted label is {}\".format(label))\n\n# initialize our gradient class activation map and build the heatmap\ncam = GradCAM(model=model, classIdx=i, layerName='conv5_block3_out')\nheatmap = cam.compute_heatmap(image)\n# resize the resulting heatmap to the original input image dimensions\n# and then overlay heatmap on top of the image\nheatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n(heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n\ncv2.imshow(\"Original\",orig)\ncv2.imshow(\"Heatmap\", heatmap)\ncv2.imshow(\"Overlay\", output)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:32:05.283002Z","iopub.execute_input":"2022-03-26T23:32:05.283504Z","iopub.status.idle":"2022-03-26T23:32:06.120715Z","shell.execute_reply.started":"2022-03-26T23:32:05.283465Z","shell.execute_reply":"2022-03-26T23:32:06.119388Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion & Improvement","metadata":{}},{"cell_type":"markdown","source":"- The pretrained model was already very good. Fine tuning does not really seem to help. It might be more interesting to introspect the quality of the labeling in the training set to check for images that are too ambiguous and should be removed from the training set.","metadata":{}}]}